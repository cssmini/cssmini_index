[{"title":"支付流程业务逻辑","date":"2020-03-30T01:50:02.000Z","path":"archives/26072.html","text":"Start 用户点击欠款支付进入欠款支付页面。 1.1 欠款支付页面初始化，请求服务器获取欠款信息。 1.2 有欠款信息则可继续下一步，否则提示【未查询到欠款信息】。 用户点击欠款支付按钮，前端请求服务器生成【业务系统订单】发起微信支付。 ​ 2.1 判断是否存在【已支付】，但未及时更改订单状态为【已完结】的订单。 ​ 2.2 生成业务系统订单，调用微信接口生成支付参数，返回给页面。 ​ 2.3 页面拉起支付，等待用户输入密码完成支付。 用户支付完成 ​ 3.1 页面进行轮询操作，查询订单状态。十秒后订单仍然未变更为【已完结】状态，则停止轮询，提示用户：订单状态同步稍有延时，请稍后再来查看。 4.支付流程完毕 用户支付完成后，后端进行的操作： 等待回调 1.1 收到回调后修改订单状态为【已支付】 1.2 并调用接口同步一条欠款支付的财经信息给业务系统。 两个定时器，定时执行操作。 订单状态：待支付：订单已经生成，等待用户支付。 已支付：（订单已经支付），通过【回调】进行修改或定时任务调用【微信查单】接口，确认用户是否支付，微信支付已经完成，修改为此状态。 已完结：微信支付已经完成，业务系统已更新欠款信息，修改为此状态。 已超时：设置一个待支付订单超时时间，判断处于待支付状态的订单，是否超过【待支付超时时间】，超时修改为此状态。 已退款：通过定时器轮询，将已支付，但超过一定时间，始终没能更改订单状态为【已完结】的订单，进行自动退款操作，同时更改订单状态为已退款。 （考虑可以不用退款，而是重新发起更新业务系统欠款支付信息的操作，同步业务系统订单，更新成功后修改订单状态为已完结） 两个定时器： 未支付订单，调用微信查单接口同步是否已支付。修改超时未支付的订单，改为已超时状态。 已支付订单，但超过一定时间后仍未修改为【已完结】状态的订单，再次尝试同步订单信息为【已完结】，或者进行退款操作。 Tips:读者可能重复支付的问题：通过判断订单状态，如果处于已支付状态，提示：用户等待系统同步欠款支付信息，请勿重复操作。 回调 + 微信查单定时器可以保证订单及时更改支付状态。 以上业务逻辑的接口列表1.获取欠款信息入参：读者id，馆代码 2.生成一条欠款待支付订单入参：读者id，馆代码，微信发起支付返回的相关参数， 3.修改订单状态 （已支付、已完结、已超时）入参：读者id，馆代码 3.1 修改订单为已支付3.2 修改订单为已完结 （用户欠款数归零）3.3 修改订单为已超时 4.获取欠款支付订单信息入参：订单 id 5.获取所有已支付订单（所有馆，返回map，key为馆代码，value为List&lt;已支付订单&gt;） 6.获取所有待支付订单（所有馆，返回map，key为馆代码，value为List&lt;待支付订单&gt;）","tags":[{"name":"支付","slug":"支付","permalink":"https://blog.cssmini.com/tags/%E6%94%AF%E4%BB%98/"},{"name":"微信","slug":"微信","permalink":"https://blog.cssmini.com/tags/%E5%BE%AE%E4%BF%A1/"}]},{"title":"微信支付提示当前页面未注册","date":"2020-03-19T03:44:25.000Z","path":"archives/27395.html","text":"单页应用微信浏览器不能正确的刷新当前页面的 href。 解决办法是进入支付页面后强制刷新一次当前页面。 这里需要注意的一点是，在微信支付的后台配置支付路径时，注意他要求配置的是目录！！ 意思是如果你的支付页面是 /pages/pay/pay 那你应该配置的目录为 /pages/pay/ 目录末尾有斜杆! ps：坑。 下面是代码： 12345678910111213141516171819202122232425onLoad(options) &#123; // 1. 初始化页面数据，获取当前欠款、预付金（仅作展示用） util.loading('初始化支付...'); // 延迟 0.5 秒再刷新页面。不然刷新不起作用 setTimeout(this.reloadUrl,500);&#125;,methods: &#123; reloadUrl()&#123; let key = 'wxPay_flag'; let that = this; if (!cache.get(key)) &#123; // 设置缓存标识 cache.put(key,'flag'); that.newHref = 'rm'; uni.hideLoading(); that.initPage(); // 2. 获取 js api 签名 that.getSign(); &#125;else&#123; cache.remove(key); // 刷新页面 window.location.reload(); &#125; &#125;,&#125;","tags":[{"name":"微信支付","slug":"微信支付","permalink":"https://blog.cssmini.com/tags/%E5%BE%AE%E4%BF%A1%E6%94%AF%E4%BB%98/"}]},{"title":"微信登录流程笔记","date":"2020-03-18T09:00:19.000Z","path":"archives/23998.html","text":"准备工作 项目集成 WX 微信SDK 配置公众号：公众号 IP 白名单、公众号设置=》JS接口安全域名、网页授权域名 微信获取 OPENID 的三次重定向 用户进入项目首页，页面上面判断本地缓存中是否存在 token，不存在则进入登录流程。 windows.loaction = 登录地址（authorize）+ &amp;当前访问项目的标识 = XXX 登录地址（authorize）接收到用户请求 根据项目标识，查询对应公众号的配置信息，set 进 wxMpService 项目标识、其他参数（如果有需要）存入 redis 生成访问微信获取 code 的 url，state 参数的值为 第二步 redis 的 key redirect 重定向 url （第一次重定向） 微信根据第二步携带的重定向地址，在处理完成后携带 code、state 重定向回我们设置的地址（第二次重定向） 验证微信重定向回来时携带的参数并进行相应的处理。 验证 code ，并且获取 openid 等信息 根据 state 读取 redis 获取我们需要的参数 根据 1、2 步的数据、业务逻辑生成 token 将 openid、业务逻辑相关的数据保存到 token 中 生成重定向回项目的地址，并且在地址上携带 token参数（第三次重定向） 用户浏览器上（微信），我们的项目首页检测到链接中携带有 token，将 token 保存到本地缓存。 微信获取 OPENID 进行登录的流程执行完毕。","tags":[{"name":"笔记","slug":"笔记","permalink":"https://blog.cssmini.com/tags/%E7%AC%94%E8%AE%B0/"}]},{"title":"SpringBoot、SpringCloud、dubbo、zookeeper之间的区别","date":"2020-03-18T03:54:00.000Z","path":"archives/29416.html","text":"转载出处 作者：ssttIsme 我做过SpringBoot和SpringCloud的项目。我们项目中使用很多技术来构建我们的项目。 SpringBoot来构建我们的项目，使用SpringCloud相关技术来实现微服务架构。 SpringBoot是一个基于maven的项目构建工具。目前我们使用版本比较稳定的1.5.6.项目后期我们项目组使用STS，Spring集成eclipse。 使用SpringCloud微服务框架，我们项目组之前还有dubbo+zookeeper。微服务框架内容非常广泛，我们项目中使用Eurka注册中心，Ribbon前端负载均衡(在本地有缓存-转向服务的列表-该列表随着Eurka更新)。 Hystrix断路器或者熔断器，Zuul实现api网关，SideCar实现异构开发语言支持。SpringCloud原生ConfigServer配置中心，Git作为分布式配置中心的数据存储的地方。 我们项目中使用Eurka作为注册中心。而没有使用dubbo使用的zookeeper作为注册中心。 我们基于分布式系统设计的定理，CAP原则。C一致性，A可用性（可用性不等于可靠性），P分区容错性。基于分布式的系统必须实现P分区容错性，一般设计要么侧重AP,要么侧重CP。 Zookeeper和Eureka的区别？ Zookeeper侧重CP设计，强调一致性。Zookeeper结构是主从结构，有leader,其他节点都是follower。当在Zookeeper节点宕机个数超过一半时和zookeeper集群在选举时，zookeeper是不推荐使用的。 Eureka侧重AP设计，强调可用性。Eurka结构点对点。每个节点互为主从，即是主节点，又是从节点。每个节点都会同步所有的数据。Eurka数据都在内存中，底层就是concurrentHashMap,防止高并发线程安全。多级缓存，4级。应对高并发。Eureka心跳机制，心跳从client端，每隔30s向服务端发起心跳请求，顺便检查数据是否变化。如果没有变化，等待下一个周期来发起心跳。如果服务列表变化，就从Eurka中同步列表。即使所有的节点都宕机，服务依然可用，client在发起端，它是EurkaClient，本地缓存之前路由，直接转向。 Feign封装Rest支持，对外体现是一个接口，目前来说比RestTemplate方式好用。具体Feign就是写一个接口。但是这个接口的坑非常多。 SpringCloud号称整合Feign之后完全兼容SpringMVC.实际不是完全兼容，差异很大 @RequestMapping,@Get,@Post,Feign目前不支持 @RequestMapping(“/aaa/bbb”)controller中可以分成两个段，但是Feign类上就不能写了 简单参数，@PathVariable(“name”)必须写全，controller可以省略 对象参数，controller直接支持对象参数，Feign不支持，@RequestBody以json接参，内部才能动态传入到对象中。 时间bug+ObjectMapper(jackson没做可配置)，两个解决方案，feign接参字符串，拦截器单独写。 Ribbon前端负载均衡 nginx后端负载均衡，nginx按配置转向，它不能感知服务死活，直接转向，有可能超时。 ribbon有本地客户端，转向信息保存在本地缓存中，直接从这个缓存中选中路径，转向(没到服务端就选择路径-按轮询算法选择)。Ribbon能感知服务死活，因为它Eureka，Eureka是动态维护服务列表的。(Ribbon从Eurka更新，只要Eurka正常，不正常的服务就通过心跳被去掉，没有那个列表就不会导致导向错误甚至超时) 请求-》Zuul网关-》nginx-》ribbon Hystrix断路器能实现限流，降级-不重要的服务少导向，甚至停止，走断路器返回默认值。Hystrix三种状态:关闭，半开，打开。 断路器在业务正常访问时，处于”关闭”状态，当业务访问失败时，超过预期值阀值，然后才切换打开状态。回调fallback断路器方法，这个方法返回默认值(预案)，返回json对象。不是一直失败，一直打开。当用户发现微服务异常，修复异常。微服务章程访问。它每次失败时先试试能否正常访问，一旦测试业务访问正常，会毫不犹豫地从半开状态切换到关闭状态。 和Zuul整合后，必须按照Zuul方法去调用，实现接口。 传统编程中以前没有这方面的内容，抛出异常。 Zuul gataway API网关，认证授权，类似shiro。转发类似nginx。 所有用户的请求都必须通过它来中转。 三条通路:服务提供者，服务消费者，Zuul。实际开发中部署，Zuul在外网暴露，提供者和消费者不暴露(内网)。 消费者可以增强我们的程序，无需自己写. SpringCloudConfig配置中心(轻量级,速度快) 替代本地属性文件解决方案，如果服务器个数非常多，100+。如果每台服务都去手动修改，在很长的时间内数据不一致。手动修改，会遗漏。配置中心就把单机的配置升级到分布式的环境中。每台机器都能访问配置中心。SpringCloudConfig是利用git存储，天然就是分布式。 ZooKeeper配置中心，它内部是小的树形结构，称为数据字典，字典很小。这个结构会在不同的节点，自动地同步，zookeeper主节点的数据从节点自动备份，所有节点数据互相联通。因为量小，zookeeper同步速度非常快。Hadoop的配置信息就是由Zookeeper来管理的。 SpringCloud和dubbo比较 SpringCloud 内容繁多，形成SpringCloud全家桶，利用SpringBoot来构建程序更跨界。SpringCloud基于http协议,(纯文本)RESTFul+json(序列化json)返回 2.Dubbo阿里开源，2003年在双11的产品，这个产品是经过极大实际使用的，而且在阿里运行6年。一开源，当当提升spring版本和rest支持dubbox.携程使用dubbo。Dubbo只是SOA的最佳实践，是微服务框架。底层基于TCP协议，RPC（基于二进制,谷歌的protobuf/thirft-性能更高的压缩二进制协议）方式-也叫二进制序列化(把对象转为可传输的东西)。 性能上SpringCloud不如dubbo。而json天然异构。 远期谷歌gRPC的要颠覆网络协议。谷歌先是默认https协议，现在要完全基于新的rpc方式。(Http协议不如RPC快) 目前为止，dubbo宣称是SpingCloud的一个子集，专注于RPC的传输方式。 面向？ 面向过程，日常搭建开发的代码都是面向过程的为了兼顾性能没有完全面向对象。mybais开发基于sql，面向过程。 面向对象，开发中需求变更时能减少代码修改量，不能完全克服需求变更。 面向函数，大数据的思考方式:面向对象+面向函数。jdk8也推出函数式编程。 面向架构，SOA面向架构编程。SOA有一个重要概念:BUS总线，BUS最早想实现所有的按照标准的模业务模块式插入BUS总线中。如办公自动化和订单还有物流都可以接入总线。各个业务模块通过总线通信。梦想：一个大大的软件系统就OK。用友的u8,金蝶是中国财务软件底层构建了soa,但是自身性能不足。 面向服务，微服务兴起，基于SOA概念，是SOA的降级。没有那么庞大。微服务是一个标准。它的标准是:Rest+JSON或RPC,springCloud选择前者(Rest+JSON+MQ),dubbo走了后者(基于RPC) Dubbo是SOA的最佳实践","tags":[{"name":"Spring","slug":"Spring","permalink":"https://blog.cssmini.com/tags/Spring/"},{"name":"dubbo","slug":"dubbo","permalink":"https://blog.cssmini.com/tags/dubbo/"},{"name":"分布式","slug":"分布式","permalink":"https://blog.cssmini.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"什么是数据埋点","date":"2020-03-18T03:50:11.000Z","path":"archives/23552.html","text":"出处：数据分析产品的下一个进化：基于无埋点的有埋点 一直以来，人们把大数据和埋点技术紧紧捆绑在一起，大数据时代也被称为埋点时代。技术发展，更新更快的无埋点技术横空出世。那么埋点技术是不是就此被判了死刑，无埋点就是万能的了？ 非也，二者只会进化的更为高级。 为什么这样说呢？其实从埋点技术的诞生和发展不难看出，一切都是源于大数据的发展，对数据的需求更加全面和精准，为适应这种发展，埋点技术不断更新迭代。这也是大数据发展的根源。 埋点的进化发展史 互联网发展起始阶段，用户不关心流量，没有意识到需要检测网站信息，一切都处在野蛮生长的阶段，随着时代的进步，业务也在增长，网站的流量开始增多，这时大家意识到这些数据中蕴含着大量的用户信息，加之用户需求越来越复杂，这时运营人员就需要一些关键的数据作为参考。 比如一些互联网公司，发展到一定程度，都会有专门的数据团队或者兼职数据人员，对公司的一些业务指标负责。同时产品的迭代升级同样需要大量的数据来支撑，如果没有数据指标的支撑，又怎么衡量这个功能升级是不是合理的呢？互联网产品并不是功能越多就越好，产品是否经得起用户考验，还是要基于数据说话的，然后学习新知识，用于下一轮的迭代。于是，埋点就此诞生了！ 从埋点发展到今天的无埋点经历三个阶段的升级。 1 代码埋点 第一阶段是代码埋点，最初的埋点是在代码的关键部位植入N行代码，追踪用户的行为,得到想要的数据。挖开产品本身,找到收集点.进行源源不断的传递数据。简单的说,找节点,布代码,收数据。 但随着业务规模扩大，数据需求增多，埋点效率低下，采集成本过高等问题开始暴露，这时候新的埋点技术出现了，即第二阶段框架式埋点。 2 框架式埋点 框架式埋点也称“可视化埋点”。用框架式交互手段来代替纯手工写代码，固化相应代码的做为SDK,方便直接调用.这是一个非常大的进步。框架式埋点很好地解决了代码埋点的埋点代价大和更新代价大两个问题。但框架式埋点能够覆盖的功能有限，关键在于不是所有的控件操作都可以通过这种方案进行定制，而且数据收集难度加大，因此无埋点技术走入了大众的视线。 3 无埋点“无埋点”则是先尽可能收集所有的控件的操作数据，然后再通过界面配置哪些数据需要在系统里面进行分析。“无埋点”相比框架式埋点的优点，一方面是解决了数据“回溯”的问题，另一方面，“无埋点”方案也可以自动获取很多启发性的信息。 无埋点大大减少了开发人员的开发成本及技术和业务人员的沟通成本。可以说无埋点技术的出现，最大化的提升了数据收集的速度，大幅缩短了数据收集周期，使得原来不敢想的事情现在敢做了，原来碍于必须有时效性不敢收集的数据也可以迅速进行分析了，在这点上，无埋点技术对传统埋点技术的优势巨大。那么发展到无埋点是否就此为止了呢？答案是否定的。 无埋点上的有埋点 从埋点到无埋点，每个阶段的演变都是顺应时代发展的需求，二者不是简单的被淘汰，而是在原来的基础上更新迭代，回到根源上来说，对数据的全面和精准，也是技术进化的一个催化加。因此我们有理由大胆猜测，数据分析技术只会继续下一个阶段的进化，基于无埋点上的有埋点，支持我们的理由是什么？ 首先我们了解一下它的概念，所谓无埋点技术，并不是说完全不用在App中植入代码，而是需要调用SDK代码，在应用页面的加载过程中、点击事件传播过程中、在其中间的某个点自动嵌入监测代码来采集数据。简单来说，就是通过简单的引入一段代码来实现监测。 目前主流的APP监测，引入监测方的SDK；网站端监测，则引入监测方的JS文件，通过引入的SDK或者JS文件来实现对APP或者网站的流量、页面热点、用户数等等这类基础数据的统计分析。因此无埋点，并非完全不埋点，只是少埋点，不是大家理解的不埋任何代码就能实现监测，无埋点不能脱离有埋点独立存在的。 其次，虽然无埋点看似十分先进，但也同样存在一些弊端，不能灵活地自定义属性，传输时效性和数据可靠性欠佳，由于所有的控件事件都全部搜集，给服务器和网络传输带来更大的负载；现有的无埋点技术并非官方标准方案，有可能在未来无法使用；监测需求相对比较基础，更多的是依据流量、用户、热点的一些分析统计，不涉及到一些自定义、或者更细化的统计分析，比如每个订单、 会员的监测；或者页面存在jQuery时对页面热点的监测。 比如我们以APP来说，APP所有新闻页、产品详情页的类名都是一个，那么无埋点就无法区分不同新闻页或者产品详情页的数据，这就影响到了数据的精准，这种情况下就需要添加代码来实现。 就比如城市要铺设新的业务管道，那必须开挖路面，光看是不行的，同时要计量或控制管道的流量大小，知道管道里的流动情况，就必须在相应的节点上装相应的阀门，这就好像埋点一样。 有时一些特殊需求或者特殊格式，也需要额外手动发送请求代码来实现，比如滚动条高度、及其它稍复杂的监控都无法做到，如果需要采集全方位的数据进行更专业的分析，仍需要靠开发人员来埋点配置。 可见，无埋点在数据监测中并不能做到全面。这就注定埋点技术不会安安静静的选择“狗带”，无埋点技术又不是吹嘘的十分万能。实现全面监测，将二者有效的结合在一起才是发展的正理。 因此在基于无埋点的基础上，通过一些手动发送请求方式（也就是所谓的埋点），来实现全面监测，这是目前行业需求和技术发展的主流方向，这种技术既解决了数据分析中的弊端，又确保了数据的精准性，同时也具备很强的扩展性。 我们静待下一个进化阶段的到来！ 数据埋点是什么？设置埋点的意义是什么？原文出处https://www.zhihu.com/question/36411025 作者：国双商业市场链接：https://www.zhihu.com/question/36411025/answer/144973846来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 所谓“埋点”，是数据采集领域（尤其是用户行为数据采集领域）的术语，指的是针对特定用户行为或事件进行捕获、处理和发送的相关技术及其实施过程。 埋点的技术实质，是先监听软件应用运行过程中的事件，当需要关注的事件发生时进行判断和捕获，然后获取必要的上下文信息，最后将信息整理后发送至服务器端。所监听的事件，通常由操作系统、浏览器、APP框架等平台提供，也可以在基础事件之上进行触发条件的自定义（如点击某一个特定按钮）。一般情况下，埋点可以通过监测分析工具提供的SDK来进行编程实现。 埋点的业务意义显而易见，即帮助定义和获取分析人员真正需要的业务数据及其附带信息。在不同场景下，业务人员关注的信息和角度可能不同。典型的应用场景有面向数字营销领域的分析，以及面向产品运营领域的分析。前者注重来源渠道和广告效果，后者更在意产品本身流程和体验的优化。两者各有侧重，也可以有一些交叉。所以，对于不同的项目和分析目的，应当设计不同的埋点方案。 近年来，埋点的方法论上也出现了一些业界新趋势，如“无埋点”技术。所谓“无埋点”，是指不再使用笨拙的采集代码编程来定义行为采集的触发条件和后续行为，而是通过后端配置或前端可视化圈选等方式来完成关键事件的定义和捕获，可以大幅提升埋点工作的效率和易用性。在“无埋点”的场景下，数据监测工具一般倾向于在监测时捕获和发送尽可能多的事件和信息，而在数据处理后端进行触发条件匹配和统计计算等工作，以较好地支持关注点变更和历史数据回溯。当然，即便是“无埋点”技术，也仍然需要部署数据采集基础SDK（又称基础代码），这一点需要注意，容易产生误区。 如果需要了解更多关于埋点的详细信息，可以阅读宋星的文章： http://www.chinawebanalytics.cn/auto-event-tracking-good-bad-ugly/ 作者：马天云链接：https://www.zhihu.com/question/36411025/answer/133787143来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 数据埋点分三个阶段：初级的数据埋点：在产品流程关键部位植相关统计代码，用来追踪每次用户的行为，统计关键流程的使用程度。 中级的数据埋点：在产品中植入多段代码追踪用户连续行为，建立用户模型来具体化用户在使用产品中的操作行为。 高级的数据埋点：与研发及数据分析师团队合作，通过数据埋点还原出用户画像及用户行为，建立数据分析后台，通过数据分析、优化产品。 埋点的意义： 数据埋点为了统计分析的需要，对用户行为的每一个事件进行埋点布置，并对这些数据结果进行分析，进一步优化产品或指导运营。 作者：大头鱼 链接：https://zhuanlan.zhihu.com/p/25195217 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 所谓埋点就是在应用中特定的流程收集一些信息，用来跟踪应用使用的状况，后续用来进一步优化产品或是提供运营的数据支撑，包括访问（Visits），访客（Visitor），停留时间（Time On Site），页面查看（Page Views，又称为页面浏览）和跳出率（Bounce Rate，又可称为蹦失率）。这样的信息收集可以大致分为两种：页面统计（track this virtual page view），统计操作行为（track this button by an event）。 数据埋点的方式现在埋点的主流有两种方式：第一种：自己公司研发在产品中注入代码统计，并搭建起相应的后台查询。第二种：第三方统计工具，如友盟、百度移动、魔方、App Annie、talking data等。 如果你的数据来自第二种，那你使用的工具也应该是第三方统计工具，后续没啥数据产品了，好好用这些产品吧。这里说说第一种的埋点方式吧，怎么数据埋点，就需要根据自己产品的任务流及产品目标来设计。 关键指标我们先看看无论是APP还是H5都会关注的指标，了解这些指标的计算方法的细微差异以及复杂性，换个角度来思考埋点的意义。【源自：精通Web Analytics 2.0】 访问与访客访问（Visits）与访客（Vistors）是几乎所有应用都需要统计的指标，这也是最基础的指标。 对于应用的统计来说，希望统计的是访客（Vistors）。访问（Visits）是指会话层，用户打开应用花一段时间浏览又离开，从指标定义来说这杯称之为一个会话（Session）。一次会话（Session 或 Visit）是打开应用的第一个请求（打开应用）和最后一个请求决定的。如果用户打开应用然后放下手机或是离开电脑，并在接下来30分钟内没有任何动作，此次会话自动结束，算作一次访问或会话期。 在计算访客时，埋点上报的数据是尽可能接近真实访客的人数。对于独立访客这个指标，这里还是需要强调一下，独立访客数并不是真实独立的人，因此收集数据时必须知道独立访客虽然能够很好的反映使用应用的真实访问者的数量，但不等于使用应用的真实人数。 原因是，重复安装的应用，或是手机参数被修改都会使得独立访客的指标收到影响。独立访客的埋点都是依赖Cookie，用户打开应用，应用都会在此人的终端创建一个独立Cookie, Cookie会被保留，但还是难免会被用户手动清理或是Cookie被禁用导致同一用户使用应用Cookie不一致，所以独立访客只能高度接近于使用应用的真实人数。 停留时长停留时长用来衡量用户在应用的某一个页面或是一次访问（会话）所停留的时间。 页面停留时长，表示在每个页面所花费的时间；例如：首页就是进入首页（10：00）到离开首页进入下一个页面(10:01)的时长，首页停留时长计算为1分钟。页面A是2分钟。页面B进入时间（10：03），离开时间没有记录，这时候计算就是0 ，这种特殊情况的处理是需要在埋点特别注意的，还是那句话，不要尝试收集绝对精准的数据，要学会使用不全的数据，活学活用。 应用的停留时长，表示一次访问（会话）所停留的时间，计算起来就是所有页面的访问时长，同样是上一个流程，应用的停留时长就是4分钟。 跳出率跳出率的计算方法现在在各个公司还是很多种，最精彩被使用的是：单个页面访问的所占的会话比例。这种场景意味着用户来了访问了一个页面就离开了，想想用户使用的心里画面应该是：打开应用，心想什么鬼，然后关闭应用甚至卸载了。这个场景多可怕，这也是为什么跳出率指标被如此关注。 跳出率可以分解到两个层次：一是整个应用的跳出率，二是重点的着陆页的跳出率，甚至是搜索关键词的跳出率。跳出率的指标可操作性非常强，通过统计跳出率可以直接发现页面的问题发现关键词的问题。 退出率退出率是针对页面的，这个指标的目标很简单，就是在某个页面有多少用户离开这个页面，主要用户反映用户从应用离开的情况。哪些页面需要被改进最快的方式被发掘。（有些流程中设定走完标准流程，退出率最高的在标准流程的最后的页面反映的正向呢，不要认为退出率高都是坏的事情哦） 转化率我们在产品上投入这么多，不就是为了衡量产出么？所以对于电商类应用，还有比转化率更值得关注的指标吗？转化率的计算方法是某种产出除以独立访客或是访问量，对于电商产品来说，就是提交订单用户数除以独立访客。 转化率的计算看起来想到那简单，但却是埋点中最贴近业务的数据收集。这也是最体现埋点技巧的指标，需要结合业务特点制定计算方法。提交订单量/访客数是最基本的转化率，转化率还可以分层次，指定用户路径的，如：完成某条路径的提交订单数/访客数。 试着找一条路径，想想转化率的数据怎么得来的吧，埋点都收集了什么样的数据吧？ 参与度参与度并不是一个指标，而是一系列的指标，访问深度，访问频次这些都是衡量参与度的指标。之所以把参与度列为一个指标，是希望大家明白把指标组合后续产生化学反应，发现实物的本质。 埋点的内容看完关键的这些指标后，有没有发现埋点的来源也大致分为两部分，一部分是统计应用页面访问情况，即页面统计；另外一部分是统计应用内的操作行为，及自定义事件统计。 页面，事件都被唯一标记，用户的信息，设备的信息，时间参数被附加上报。这就是埋点。 关于埋点的数据的注意事项不要过分追求完美关于埋点数据有一点至关重要，埋点是为了更好地使用数据，不要试图得到精准的数据要得到的是高质量的埋点数据，前面讨论跳出率就是这个例子，得到能得到的数据，用不完美的数据来达成下一步的行动，追求的是高质量而不是精确。这是很多数据产品容易入坑的地，要经常提醒自己。 作者：赵素卫链接：https://www.zhihu.com/question/36411025/answer/139101494来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 数据埋点是一种良好的私有化部署数据采集方式。数据采集准确，满足了企业去粗取精，实现产品、服务快速优化迭代的需求。 简单的说：找节点, 布代码, 收数据。 数据埋点可以分为三个阶段： 代码埋点 、 框架式埋点 （也称为可视化埋点）以及无埋点。 但随着业务规模扩大，数据需求增多，埋点效率低下，采集成本过高等问题开始暴露， 越来越多的公司开始注重无埋点技术。 曾经在一个公众号中，看过一篇文章，写的不错，推荐给您！我上面说的，大多也是摘自这里 https://mp.weixin.qq.com/s/VSIQ9SuizaBo8KTCXQxZkQ 数据分析第一步–做好数据埋点 做产品的同学在产品上线后经常离不开一个词，数据分析。那么要如何进行数据分析呢？不妨先问自己这么几个问题。1.你要分析什么问题？是找问题还是验证？2.关于这些问题你需要哪些数据？3.这些数据从哪里来？ 要怎么解决这些问题呢？答案是数据埋点。首先通过产品定位及目标来确定自己需要哪些数据，其次通过在产品各个流程环节中设置数据埋点，最后，当用户使用产品时，后台就能源源不断地接收到数据了。 那么，问题又来了。如何做好数据分析的第一步，数据埋点呢？还是从三个问题来回答1.数据埋点是什么？初级的数据埋点：在产品流程关键部位植相关统计代码，用来追踪每次用户的行为，统计关键流程的使用程度。中级的数据埋点：在产品中植入多段代码追踪用户连续行为，建立用户模型来具体化用户在使用产品中的操作行为。高级的数据埋点：与研发及数据分析师团队合作，通过数据埋点还原出用户画像及用户行为，建立数据分析后台，通过数据分析、优化产品。 2.为什么要做数据埋点？一个简单的逻辑：你不做数据埋点，你就做不了数据分析。你不做数据分析，你就会不知道产品上线情况。你不知道产品上线情况，你产品就会做差。你产品做差，你的业绩就会不好。你业绩不好你就会被辞，你被辞就会没钱。你没钱就会去睡马路。你睡马路你就可能会被车撞，你被车撞就会…所以为了不被车撞，一定要做好数据埋点！ 3.怎么做好数据埋点？（1） 数据埋点的内容![Upload 数据埋点 (1).png failed. Please try again.]数据埋点可以分为产品内部埋点和市场埋点，内部埋点通常分析用户使用产品的行为及流程，提升用户体验。市场埋点分析该产品在市场上的表现及用户使用场景，如产品在不同市场和地域的下载量，不同地域人群使用时间等等。 产品流程通常分为主干流程和分支流程，所以相应的数据埋点可以分为主干埋点和分支埋点，数据埋点通常不会一步搞定，在产品的第一次上线时通常会埋以下几个点：PC&amp;Web端会统计产品的PV/UV，注册量，主要流程页面之间的转化率、日活人数等等。而移动端还要统计产品在Appstore，各大安卓市场的下载量。 第二次埋点会根据产品目标及上线后的问题进行分析。比如，当你发现产品首页的UV很高， 注册量却非常低，你就需要分析出用户在首页的行为，如30%的用户退出了产品，60%的用户进入了注册页，但只有1%的用户注册了该产品。这也就意味着，注册流程可能出现了问题，需要进一步细化注册各个流程，增加数据埋点，分析各个流程之间的转化率，找到产品出现的问题并解决。 具体到自己的产品，怎么数据埋点，就需要根据自己产品的任务流及产品目标来设计。这是一个由粗到细，优化迭代的过程。 （2）分析方法任务流程分析法：根据产品设计的任务流，在任务流开始和结束处埋点，分析用户处理任务的情况。页面转化分析法：统计相关页面的转化率及页面元素点击率，分析用户行为。情景分析法：列出各种用户使用场景，自己或多人体验不同场景下产品的使用流程，寻找依据设立数据埋点，通过数据反馈验证用户行为。 （3）数据埋点的方式目前主流的数据埋点方式分为两种：第一种：自己公司研发在产品中注入代码统计，并搭建起相应的后台查询。第二种：第三方统计工具，如友盟、百度移动、魔方、App Annie、talking data等。最后，还是要说，数据埋点是产品数据分析的基础，也是个循序渐进的过程。基础的数据分析并不难，让数据来驱动产品迭代。 作者：placeless链接：http://www.jianshu.com/p/8c491348d2ba來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","tags":[{"name":"数据埋点","slug":"数据埋点","permalink":"https://blog.cssmini.com/tags/%E6%95%B0%E6%8D%AE%E5%9F%8B%E7%82%B9/"}]},{"title":"HTML加密混淆","date":"2020-03-18T03:40:54.000Z","path":"archives/19273.html","text":"转载 关于HTML、js加密、混淆、源码保护、代码安全，防止解压直接看源码 一直有人问HTML加密混淆怎么做，其实这在业内是早已很多人研究过的课题。假日期间整理一篇文章分享给大家。 我们先理下需求，加密的目的是什么？加密到什么级别？为此我们可以牺牲什么？我们知道这个世界不存在绝对的安全，加密会被破解、混淆会被反混淆。技术小白、开发者、黑客，是完全不同的级别，防范不同级别的人策略都不一样。防范力度越大，投入代价也越大，比如聘请专业的安全公司。除了投入，我们还需要考虑程序的执行性能和用户体验。加密的代码在运行时必须解密，混淆后尤其是混淆HTML后，程序的执行性能会下降。是否真的有必要做这类的源码保护，还需要谨慎取舍。 一般而言，前端的代码，负责的是用户体验，后端的代码，负责更安全的数据处理。前端不要涉及泄漏太多涉密信息，那么加密的意义不是特别大。我很少在前端代码里看到值得保护的内容，比如高深的算法，很多代码是没必要牺牲用户体验来保护的。但有些前端代码涉及最终用户的数据安全，此时还是要努力做数据保护的。 接下来具体分析几种手段。不要在前端放敏感数据这个听起来是废话，但真的很重要。有些开发者在手机端明文存用户的密码，这是非常危险的事情。即使是原生开发，一旦手机被root，也会造成数据泄漏。更何况HTML5开发。比较好的做法是手机端存token，而不是密码，这里有篇文章专门介绍这块，涉及做登录的开发者推荐仔细看看设计基于HTML5的APP登录功能及安全调用接口的方式（原理篇） js、css压缩压缩不是加密，也不是混淆。但压缩后的js文件，往往也具有混淆的功能。js、css压缩是很常见的技术，我们经常看到各种框架的文件名是xxx.min.js，xxx.min.css。使用合适的js、css压缩方案，可以减少文件体积、提高载入速度，最重要的是，它还能加快程序的执行性能。简直是有百利而无一害。压缩js比较常用的工具是yahoo的YUI混淆，在HBuilder里点菜单工具-插件安装，里面有YUI compress，可以压缩js和css。如果js、css比较大，发布前压缩下是比较推荐的做法。 HTML、js、css混淆压缩虽然也能混淆，但不是以让别人看不懂为目的，混淆是真正以别人看不懂为目的。但是这样的混淆就不像压缩那么有百利而无一害了，它会降低程序执行性能。一些开发者不希望发行包解压后可以直接看到源码，那么此时可以使用混淆方案。 网上搜索HTML混淆，资料和工具都非常多。原理都是类似的，js代码变成乱七八糟的字符串，然后用eval执行，HTML代码变乱七八糟字符串，用document.write或innerHTML执行，css也可以动态的在document.write里写 &lt;style&gt;。这些工具有免费也有商业的，一般越商业的越难被反混淆。 这个是免费的在线混淆工具 http://www.myobfuscate.com这个是比较知名的商业工具，http://www.jasob.com其实大家也可以根据原理自己写混淆算法。 混淆也是一个有年头的成熟技术，比如Google在保护gmail的前端代码时，也是通过混淆来保护的。不管是压缩还是混淆，使用grunt来做发布是不错的方式，开发后一键调用grunt处理，非常便捷。 HBuilder在原生层面支持js混淆，性能高于前端混淆。详情下面第6点。 这篇文章对于前端的代码保护也讲的非常专业，值得大家学习http://div.io/topic/1220 防止webkit remote debug，即防止浏览器控制台调试Android4.4及以上和iOS是支持webkit remote debug的，在HBuilder的教程里也有如何使用chrome调试Android应用，和使用safari调试iOS应用的教程。在HBuilder开发的App里，manifest.json里下的plus-distribute下有一个debug标签，标记为false后打包（可视化界面里也有配置），这样的包运行在手机上时webview会阻止浏览器的远程调试请求。 如果你想调试，那么把debug改为true后再打包。当然有些Android rom不是很规范，并不能阻止调试，这属于rom的bug。 另外注意只有打包才能防调试，真机运行是不能阻止调试的。专业加密加固加壳服务 由于Android的特殊性，针对apk出现了加固、加壳产业，这也是业内常见的apk保护方案。很多应用市场都提供加固服务，比如360手机助手的加固。还有一批专业公司如爱加密，里面有免费的基础安全保障服务，也有收费的高级安全保障服务。这里还有一个防止apk解压的小技巧：http://www.52pojie.cn/thread-287242-1-1.html HBuilder提供的原生js混淆在HBuilder或HBuilderX中，官方提供了原生层面的js混淆。这种混淆的性能比纯前端混淆的性能要更好，反编译的难度也更大。但有些限制和使用注意：Android4以下的手机不能运行加密后的js。5+App下，iOS若配置wkWebview，则不能运行加密后的js。具体使用方式是在打包界面，可以选择需要加密的js文件，然后打包即可。如下图：","tags":[{"name":"HTML","slug":"HTML","permalink":"https://blog.cssmini.com/tags/HTML/"},{"name":"加密","slug":"加密","permalink":"https://blog.cssmini.com/tags/%E5%8A%A0%E5%AF%86/"},{"name":"安全","slug":"安全","permalink":"https://blog.cssmini.com/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"设计基于HTML5的APP登录功能及安全调用接口的方式（原理篇）","date":"2020-03-18T03:39:05.000Z","path":"archives/12383.html","text":"转载 设计基于HTML5的APP登录功能及安全调用接口的方式（原理篇） 最近发现群内大伙对用Hbuilder做的APP怎么做登录功能以及维护登录状态非常困惑，而我前一段时间正好稍微研究了一下，所以把我知道的告诉大家，节约大家查找资料的时间。 你是否真的需要登录功能？把这个问题放在最前面并不是灌水，而是真的见过很多并不需要登录的APP去做了登录功能，或者是并不需要强制登录的APP把登录作为启动页。用户对你的APP一无所知，你就要求对方注册并登录，除非APP本身已经很有名气或者是用户有强需求，否则正常人应该会直接把它删掉。比较温和的方式是将一些并不需要登录，但可以给用户带来帮助的东西，第一时间展现给他们，让他们产生兴趣，再在合适的时机引导他们注册（比如使用需要使用更高级的功能，或用户需要收藏某个喜欢的信息时）。 登录和注册要足够简单这是小小的手机端，用再好的输入法，打字也是不方便的，所以别把登录页设计得需要填很多东西。如果有可能的话，只填手机号，让用户收到短信验证码就完成注册是最好不过的了。想获得更多信息？想想大公司的APP是怎么做的，他们会告诉用户，现在的个人资料完善程度是30%，如果想获得更多积分，你需要填完。tips:如果你想发布在Appstore并且同时包含注册功能，那么注册页面必须做一个用户许可协议的链接，否则有可能通不过审核。 实现登录后的session有几种方式？APP当浏览器用，直接载入远程页面这种情况是很多偷懒的程序员或者傻X的老板选择的方式，因为做起来实在太快。如果本身网站是响应式布局，那么很有可能不需要做什么更改，就只要在开发时打开首页就好了，这样Hybird的APP外壳就纯粹成为了一个浏览器。但比起这样做带来的无数缺点来，开发速度快的优点几乎可以忽略不计。首先，在网络环境不佳时，纯大白页，用户体验0；然后，CSS和JS等资源不在本地，需要远程载入，如果使用了bootstrap之类的框架，那用户为了开一下APP而耗费的流量真是令人感动；再然后，网页里常用的jquery，在手机的webview里速度并不理想，而如果是非ajax的网页那就更糟心了，每次操作都要跳转和页面渲染，要让人把它当成APP那实在是笑话。再再然后，这样的所谓APP，要通过Appstore的审查，那是做梦的（除非审核员当天闹肚子严重，拿着纸巾奔向厕所前误点了通过……），苹果的要求是，这得是APP，而不能是某个网站做成APP的样子，那样的情况适合做Web APP。而据我所知，国内几个较大的Android市场，这样的APP也是无法通过审核的。 调用后端接口这是个很好的时代，因为无论后端你是用Java、PHP，还是node.js，都可以通过xml、json来和APP通讯。遥想当年写服务端要自己写包结构，然后为了解决并发问题还折腾了半年IOCP模型，真心觉得现在太幸福了。把刚才那个用APP当浏览器使的案例的所有缺点反过来看，就是这样做的优点，在优化完善的情况下体验接近原生，而且通讯流量极少，通过各种审核也是妥妥的。tips:通过plus对象中的XMLHttpRequest来Get、Post远程的后端接口，或者使用Mui中封装好的AJAX相关函数。 调用后端接口怎么样才安全？在APP中保存登录数据，每次调用接口时传输程序员总能给自己找到偷懒的方法，有的程序为了省事，会在用户登录后，直接把用户名和密码保存在本地，然后每次调用后端接口时作为参数传递。真省事儿啊！可这种方法简单就像拿着一袋子钱在路上边走边喊“快来抢我呀！快来抢我呀！”，一个小小的嗅探器就能把用户的密码拿到手，如果用户习惯在所有地方用一个密码，那么你闯大祸了，黑客通过撞库的方法能把用户的所有信息一锅端。 登录时请求一次token，之后用token调用接口这是比较安全的方式，用户在登录时，APP调用获取token的接口（比如http://api.abc.com/get_token/），用post将用户名和密码的摘要传递给服务器，然后服务器比对数据库中的用户信息，匹配则返回绑定该用户的token（这一般翻译为令牌，很直观的名字，一看就知道是有了这玩意，就会对你放行），而数据库中，在用户的token表中也同时插入了这个token相关的数据：这个token属于谁？这个token的有效期是多久？这个token当前登录的ip地址是？这个token对应的deviceid是？…… 这样即便token被有心人截获，也不会造成太大的安全风险。因为没有用户名和密码，然后如果黑客通过这个token伪造用户请求，我们在服务器端接口被调用时就可以对发起请求的ip地址、user-agent之类的信息作比对，以防止伪造。再然后，如果token的有效期设得小，过一会儿它就过期了，除非黑客可以持续截获你的token，否则他只能干瞪眼。（插一句题外话：看到这里，是不是明白为什么不推荐在外面随便接入来历不明的wifi热点了？） tips:token如何生成？ 可以根据用户的信息及一些随机信息（比如时间戳）再通过hash编码（比如md5、sha1等）生成唯一的编码。 tips:token的安全级别，取决于你的实际需求，所以如果不是涉及财产安全的领域，并不建议太严格（比如用户走着走着，3G换了个基站，闪断了一下IP地址变了，尼玛token过期了，这就属于为了不必要的安全丢了用户体验，当然如果变换的IP地址跨省的话还是应该验证一下的，想想QQ有时候会让填验证码就明白了）。 tips:接口在返回信息时，可以包含本次请求的状态，比如成功调用，那么result[‘status’]可能就是’success’，而反之则是’error’，而如果是’error’，则result[‘errcode’]中就可以包含错误的原因，比如errcode中是’invalid_token’就可以告诉APP这个token过期或无效，这时APP应弹出登录框或者用本地存储的用户名或密码再次请求token（用户选择“记住密码”，就应该在本地保存用户名和密码的摘要，方法见plus.storage的文档）。 更安全一点，获取token通过SSL刚才的方法，机智一点儿的读者大概会心存疑虑：那获取token时不还是得明文传输一次密码吗？是的，你可以将这个获取token的地址，用SSL来保护（比如https://api.abc.com/get_token/），这样黑客即使截了包，一时半会儿也解不出什么信息。SSL证书的获取渠道很多，我相信你总有办法查到，所以不废话了。不过话说namecheap上的SSL证书比godaddy的要便宜得多……（这是吐槽）tips:前段时间OpenSSL漏洞让很多服务器遭殃，所以如果自己搭服务器，一定记得装补丁。tips:可以把所有接口都弄成SSL的吗？可以。但会拖慢服务器，如果是配置并不自信的VPS，建议不折腾。 还要更更安全（这标题真省事）还记得刚才APP向服务器请求token时，可以加入的用户信息吗？比如用户的设备deviceid。如果我们在调用接口时，还附带一个当前时间戳参数timestamp，同时，用deviceid和这个时间戳再生成一个参数sign，比如 md5(deviceid timestamp token)这样的形式。而服务端首先验证一下参数中的时间戳与当前服务器时间是否一致（误差保持在合理范围内即可，比如5分钟），然后根据用户保存在服务器中的deviceid来对参数中的时间戳进行相同的变形，验证是否匹配，那便自然“更更安全”了。tips:如果对整个调用请求中的参数进行排序，再以deviceid和timestamp加上排序后的参数来对整个调用生成1个sign，黑客即使截获sign，不同的时间点、参数请求所使用的sign也是不同的，难以伪造，自然会更安全。当然，写起来也更费事。tips:明白了原理，整个验证过程是可以根据自己的需求改造的。","tags":[{"name":"安全","slug":"安全","permalink":"https://blog.cssmini.com/tags/%E5%AE%89%E5%85%A8/"},{"name":"登录","slug":"登录","permalink":"https://blog.cssmini.com/tags/%E7%99%BB%E5%BD%95/"}]},{"title":"Nginx 常用命令","date":"2020-03-18T03:37:20.000Z","path":"archives/44716.html","text":"windows 杀进程：taskkill /F /FI “imagename eq nginx.exe” 1、启动：C:\\server\\nginx-1.0.2&gt;start nginx 或 C:\\server\\nginx-1.0.2&gt;nginx.exe 注：建议使用第一种，第二种会使你的cmd窗口一直处于执行中，不能进行其他命令操作。 2、停止：C:\\server\\nginx-1.0.2&gt;nginx.exe -s stop 或 C:\\server\\nginx-1.0.2&gt;nginx.exe -s quit 3、重新载入Nginx：C:\\server\\nginx-1.0.2&gt;nginx.exe -s reload 当配置信息修改，需要重新载入这些配置时使用此命令。 4、重新打开日志文件：C:\\server\\nginx-1.0.2&gt;nginx.exe -s reopen 5、查看Nginx版本：C:\\server\\nginx-1.0.2&gt;nginx -v","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.cssmini.com/tags/Nginx/"}]},{"title":"windows查看端口占用以及关闭相应的进程","date":"2020-03-18T03:34:20.000Z","path":"archives/54364.html","text":"出处 开始–运行–cmd 进入命令提示符 输入netstat -ano 即可看到所有连接的PID 之后在任务管理器中找到这个PID所对应的程序如果任务管理器中没有PID这一项,可以在任务管理器中选”查看”-“选择列” 经常，我们在启动应用的时候发现系统需要的端口被别的程序占用，如何知道谁占有了我们需要的端口，很多人都比较头疼，下面就介绍一种非常简单的方法，希望对大家有用 假如我们需要确定谁占用了我们的9050端口 1、Windows平台在windows命令行窗口下执行： 1.查看所有的端口占用情况 C:&gt;netstat -ano 协议 本地地址 外部地址 状态 PID TCP 127.0.0.1:1434 0.0.0.0:0 LISTENING 3236 TCP 127.0.0.1:5679 0.0.0.0:0 LISTENING 4168 TCP 127.0.0.1:7438 0.0.0.0:0 LISTENING 4168 TCP 127.0.0.1:8015 0.0.0.0:0 LISTENING 1456 TCP 192.168.3.230:139 0.0.0.0:0 LISTENING 4 TCP 192.168.3.230:1957 220.181.31.225:443 ESTABLISHED 3068 TCP 192.168.3.230:2020 183.62.96.189:1522 ESTABLISHED 1456 TCP 192.168.3.230:2927 117.79.91.18:80 ESTABLISHED 4732 TCP 192.168.3.230:2929 117.79.91.18:80 ESTABLISHED 4732 TCP 192.168.3.230:2930 117.79.91.18:80 ESTABLISHED 4732 TCP 192.168.3.230:2931 117.79.91.18:80 ESTABLISHED 4732 2.查看指定端口的占用情况C:&gt;netstat -aon|findstr “9050” 协议 本地地址 外部地址 状态 PID TCP 127.0.0.1:9050 0.0.0.0:0 LISTENING 2016 P: 看到了吗，端口被进程号为2016的进程占用，继续执行下面命令： （也可以去任务管理器中查看pid对应的进程） 3.查看PID对应的进程C:&gt;tasklist|findstr “2016” 映像名称 PID 会话名 会话# 内存使用 ========================= ======== ================ tor.exe 2016 Console 0 16,064 K P:很清楚吧，tor占用了你的端口。 4.结束该进程 C:&gt;taskkill /f /t /im tor.exe","tags":[{"name":"windows","slug":"windows","permalink":"https://blog.cssmini.com/tags/windows/"},{"name":"端口占用","slug":"端口占用","permalink":"https://blog.cssmini.com/tags/%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8/"}]},{"title":"反爬虫的机制和遇到反爬虫机制的解决方案","date":"2019-12-20T09:02:22.000Z","path":"archives/49699.html","text":"1. 什么是爬虫和反爬虫？爬虫：使用任何技术手段，批量获取网站信息的一种方式。反爬虫：使用任何技术手段，阻止别人批量获取自己网站信息的一种方式。 2. 常见的反爬虫机制​ 2.1 通过UA 识别爬虫 有些爬虫的UA是特殊的，与正常浏览器的不一样，可通过识别特征UA，直接封掉爬虫请求 2.2 设置IP访问频率，如果超过一定频率，弹出验证码 如果输入正确的验证码，则放行，如果没有输入，则拉入禁止一段时间，如果超过禁爬时间，再次出发验证码，则拉入黑名单。当然根据具体的业务，为不同场景设置不同阈值，比如登陆用户和非登陆用户，请求是否含有refer。通过并发识别爬虫 有些爬虫的并发是很高的，统计并发最高的IP，加入黑名单（或者直接封掉爬虫IP所在C段） 2.3 请求的时间窗口过滤统计 爬虫爬取网页的频率都是比较固定的，不像人去访问网页，中间的间隔时间比较无规则，所以我们可以给每个IP地址建立一个时间窗口，记录IP地址最近12次访问时间，每记录一次就滑动一次窗口，比较最近访问时间和当前时间，如果间隔时间很长判断不是爬虫，清除时间窗口，如果间隔不长，就回溯计算指定时间段的访问频率，如果访问频率超过阀值，就转向验证码页面让用户填写验证码限制单个ip/api token的访问量 比如15分钟限制访问页面180次，具体标准可参考一些大型网站的公开api，如twitter api，对于抓取用户公开信息的爬虫要格外敏感识别出合法爬虫 对http头agent进行验证，是否标记为、百度的spider，严格一点的话应该判别来源IP是否为、baidu的爬虫IP，这些IP在网上都可以找到。校验出来IP不在白名单就可以阻止访问内容。 2.4 蜜罐资源 爬虫解析离不开正则匹配，适当在页面添加一些正常浏览器浏览访问不到的资源，一旦有ip访问，过滤下头部是不是搜素引擎的蜘蛛，不是就可以直接封了。比如说隐式链接。 3. 破解反爬虫机制的几种方法策略1：设置下载延迟，比如数字设置为5秒，越大越安全 策略2：禁止Cookie，某些网站会通过Cookie识别用户身份，禁用后使得服务器无法识别爬虫轨迹 策略3：使用user agent池。也就是每次发送的时候随机从池中选择不一样的浏览器头信息，防止暴露爬虫身份 策略4：使用IP池，这个需要大量的IP资源，可以通过抓取网上免费公开的IP建成自有的IP代理池。 策略5：分布式爬取，这个是针对大型爬虫系统的，实现一个分布式的爬虫，主要为以下几个步骤： 1、基本的http抓取工具，如scrapy； 2、避免重复抓取网页，如Bloom Filter； 3、维护一个所有集群机器能够有效分享的分布式队列； 4、将分布式队列和Scrapy的结合； 5、后续处理，网页析取(如python-goose)，存储(如Mongodb)。 策略6：模拟登录—浏览器登录的爬取 设置一个cookie处理对象，它负责将cookie添加到http请求中，并能从http响应中得到cookie，向网站登录页面发送一个请求Request, 包括登录url，POST请求的数据，Http header利用urllib2.urlopen发送请求，接收WEB服务器的Response。​ 原文出处","tags":[{"name":"摘抄","slug":"摘抄","permalink":"https://blog.cssmini.com/tags/%E6%91%98%E6%8A%84/"}]},{"title":"node.js npm webPack 之间的关系","date":"2019-12-06T16:00:00.000Z","path":"archives/20529.html","text":"什么是 node.js ? 简单来说，node.js 就是一个 javascript 脚本的运行环境。它做的事情是，将 JavaScript 编译、运行等关于JavaScript 的功能从浏览器中独立了出来。因为 javaScript 具备网络通信等，服务器必备的功能。所有可以将 node.js 作为服务器来使用。而 javaScript 则是在 node.js 这个服务上运行的语言。 熟悉 node.js 可以通过 菜鸟教程 - node.js 进行学习 什么是npm ? npm 是一个运行在 node.js 之上的包管理工具，类似于 Linux 环境下的 yum 。它的作用是集中管理和发放前端的 js 插件、框架等等。便于开发者从中心服务器上下载 js 插件、框架。 npm 常用命令及作用 安装js模块npm install [packagename] [version] [-g]示例： 12// 全局安装 0.0.1 版本的 hexo 脚手架$ npm install hexo-cli 0.0.1 -g 初始化一个 js 项目npm init示例： 12// 运行命令后会引导你创建一个package.json文件，包括名称、版本、作者这些信息等$ npm init 移除 js 模块npm remove &lt;name&gt;示例： 12// 移除 hexo-cli 模块 $ npm remove hexo-cli 更新 js 模块npm update &lt;name&gt;示例： 12// 更新 hexo-cli 模块$ npm update hexo-cli 列出所有已安装的模块npm ls示例： 12// 运行命令后，会展示出所有本地已经安装了的模块$ npm ls 查看当前项目的 js 模块安装路径npm root示例： 123// 运行命令后，会展示出当前项目 js 模块安装的绝对路径$ cnpm rootH:\\temp\\node-temp\\node_modules 查看 npm 全局安装 js 模块时，文件存放的路径npm root -g示例： 123// 运行命令后，会展示出 npm 全局安装 js 模块时，文件存放的路径$ cnpm root -gD:\\Program Files\\nodejs\\node_modules package.json 是什么package.json 是执行 npm init 初始化一个项目后，自动生成的文件。它里面存放了在初始化项目时，你填写的那些信息。 当我们在这个项目下执行 npm install [packagename] js 模块安装命令时，npm 会自动将你安装模块的基本信息添加到当前项目的 package.json 中。 有了 package.json 这个文件，npm 就知道了运行这个项目需要哪些依赖模块。如果将这个项目迁移到另一台机器上时，只需要在项目路径下执行一次 npm install就可以导入所有依赖模块，使得项目正常启动。 cnpm 是什么因为国内 npm 的下载速度很慢，cnpm 是部署在国内的 npm 镜像环境。cnpm 和 npm 功能完全一样。 cnpm 配置地址 什么是 web pack ? 运行在 node.js 之上的前端打包工具，npm + web pack 可以理解为 Maven。","tags":[]},{"title":"博客搭建记录","date":"2019-12-06T16:00:00.000Z","path":"archives/41226.html","text":"博客初始化 hexo init hexo-test 更换主题 （修改配置文件 + 添加主题） 初始化必要的插件 npm install hexo-renderer-jade@0.3.0 –save npm install hexo-renderer-stylus –save 迁移到新设备保留原根目录中的 _config.yml + themes + scaffolds + source 执行博客初始化操作后，覆盖保留的文件 + 目录到新生成的博客目录下 安装 RSS订阅 插件 npm install hexo-generator-feed –save 12345678# RSS订阅支持plugin:- hexo-generator-feed#Feed Atomfeed: type: atom path: atom.xml limit: 20 安装sitmap插件 npm install hexo-generator-sitemap –save npm install hexo-generator-baidu-sitemap –save 配置文件添加： 12345# 自动生成sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 重新初始化插件 删除 node_modules 执行 cnpm install 博客搜索功能1npm install hexo-generator-json-content@2.2.0 --save 根目录配置文件添加一下配置 123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true","tags":[]},{"title":"Hello World","date":"2019-12-01T09:02:22.000Z","path":"archives/16107.html","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]},{"title":"uni-app 开发 NFC 读写功能","date":"2019-10-03T06:25:57.000Z","path":"archives/26936.html","text":"什么是NFCRFID：射频识别技术，分为接触式（需要插卡）与非接触式（只需刷卡），NFC就是从这个技术发展而来的，包含多个频段，915MHz，125KHz，13.56MHz，2.4GHz等。 NFC：近场通讯技术，只能工作在13.56MHz，所以能读取全部工作在这个频段的卡，是属于 RFID 技术的，但是又有新的功能，可以理解为 RFID 的子类 NFC 的工作原理操作通常都是主从式，即读卡器作为主动方发送命令，卡片收到命令后作出反应。 通过读卡器产生的电磁场获得能量并与读写器交换信息 常用载波频率有125KHz、134.2KHz、13.56MHz、2.45GHz等，通常载波频率越高，数据的传送速率越高，技术上也较复杂 详见 NFC 的型号NFC 厂商基于不同的国际标准，开发出的 NFC 芯片型号也不一样。 具体的有：NfcA NfcB NfcF NfcV Ndef Ndef… 在 java 中上不同的芯片型号，使用不同的类进行区分。也就是说不同的芯片有不同的读写方式。 1234ISO 14443 RFID卡标准（非接触IC卡），该标准又有很多子标准ISO 7816 接触式IC卡标准ISO 15693 NfcV NXP ICODE SLIX 卡 就是基于这个标准ISO 18092 NFC标准 出处 什么是 ISO15693 在安卓里面代表 ISO15693 协议的是 NfcV 这个类 查看相关 api 什么是 AFI 和 EAS ?这是基于 ISO15693 标准芯片的存储块名称 存储器分为32个块、每个块由4字节（32位）组成，共128字节，如下图，上部4个块（-4、-3、-2、-1块）分别用于UID（64位唯一ID序列号）、特殊功能（EAS、AFI、DSFID）和写入控制位，其他28个块为用户数据块。 每个芯片都拥有一个全球唯一id 安卓上开发 NFC 调用读取的相关示例NFC标签初始化、NFC标签读写数据功能、NFC标签前台调度系统 android nfc常用标签读取 NfcV(ISO 15693)读写操作 uni-app 开发 NFC 使用 Native.js 官方文档 DCloud 封装了一套可以通过 js 调用安卓原生 api 的代码。这个东西就是 Native.js 。但调研了一番，依旧和 uni-app 一样有很多坑。这是 DCloud 的原话： 由于NJS是直接调用Native API，需要对Native API有一定了解，知道所需要的功能调用了哪些原生API，能看懂原生代码并参考原生代码修改为JS代码。 现在我们的安卓app，实现 NFC 读写功能就是通过这个东西来实现的。 官方文档上面没有具体说明 java 上各种数据类型定义，如何通过这个 js 来进行实现。 安卓 NFC api 接口 文档","tags":[{"name":"NFC","slug":"NFC","permalink":"https://blog.cssmini.com/tags/NFC/"}]}]